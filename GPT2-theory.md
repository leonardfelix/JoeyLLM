ğŸ“˜ GPT-2 Overview: Autoregressive Model with Masked Self-Attention
ğŸ§  What Is an Autoregressive Model?
An autoregressive (AR) model predicts each token based on all the previous ones. In GPT-2:

P(xâ‚, xâ‚‚, ..., xâ‚™) = P(xâ‚) Â· P(xâ‚‚ | xâ‚) Â· P(xâ‚ƒ | xâ‚, xâ‚‚) Â·Â·Â· P(xâ‚™ | xâ‚, ..., xâ‚™â‚‹â‚)

This means GPT-2 generates text one token at a time, from left to right, and never looks ahead.

ğŸ¤– GPT-2 Architecture
GPT-2 is a generative model based on the Transformer architecture

It only uses the Decoder blocks (no Encoder)

The Decoder is repeated N times (e.g., 12 blocks for GPT-2 small)

GPT-2 is fully autoregressive, optimized for text generation

ğŸ”„ GPT-2 Tokenization and Embedding Pipeline
1. Tokenization
Input text is broken into smaller chunks called tokens using Byte Pair Encoding (BPE). Each token maps to a unique Token ID from the vocabulary V.

Example:
"Hello world!" â†’ ["Hello", " world", "!"] â†’ [15496, 995, 0]

2. Embedding Layer
Each Token ID is converted into a 768-dimensional vector using a learned embedding matrix E âˆˆ â„^{|V| Ã— 768}.

Embedding(Token ID) = E[Token ID]

This gives us:

A continuous representation of discrete tokens

Input shape: seq_len Ã— d_model

ğŸ§  Self-Attention in GPT-2
What is Self-Attention?
Self-Attention helps the model determine which tokens should pay attention to which others. It computes:

Attention(Q, K, V) = Softmax(QKáµ€ / âˆšdâ‚–) Â· V

Each input token is projected into:

Q: Query

K: Key

V: Value

These are obtained via linear transformations:

Q = X Â· W_Q
K = X Â· W_K
V = X Â· W_V

For single-head attention:

W_Q, W_K, W_V âˆˆ â„^{768 Ã— 64}

For multi-head attention (12 heads):

W_Q, W_K, W_V âˆˆ â„^{768 Ã— 768}

ğŸ§© Multi-Head vs Single-Head Attention
In Multi-Head Attention, multiple attention "heads" operate in parallel:

Each head focuses on different types of relationships (e.g., syntactic, semantic)

The outputs of all heads are concatenated:

Concat(headâ‚, ..., headâ‚â‚‚) âˆˆ â„^{seq_len Ã— 768}

Then projected back to original space using W_O âˆˆ â„^{768 Ã— 768}

Final Output = Multi-Head Output Â· W_O

Note: W_O is a trainable parameter learned during model training.

ğŸ”’ Masked Multi-Head Self-Attention in GPT-2
In GPT-2, we use Masked Self-Attention to ensure causality:

Each token can only attend to previous tokens

Future tokens are masked out using a triangular mask:

M_{i,j} =
â€ƒâ€ƒ0â€ƒâ€ƒâ€ƒif j â‰¤ i
â€ƒâ€ƒ-âˆâ€ƒâ€ƒif j > i

Applied before Softmax to prevent "seeing the future"

Why?
ğŸ‘‰ To ensure GPT-2 predicts token x_t using only xâ‚, ..., xâ‚œâ‚‹â‚

âš™ï¸ Feed-Forward Network (FFN)
After attention, GPT-2 passes the output through a two-layer position-wise feed-forward network:

FFN(x) = GELU(x Â· Wâ‚ + bâ‚) Â· Wâ‚‚ + bâ‚‚

Applies to each token independently

Allows complex feature transformation

Hidden layer size is often 4Ã— the model dimension (e.g., 3072 if d_model = 768)

ğŸ” GPT-2 Text Generation Loop
To generate a sentence:

Input: "The weather"

Predict: "is"

New input: "The weather is"

Predict: "nice"

Repeat until stop condition

Each step:

Embeds current input

Computes Masked Self-Attention

Uses output to predict next token

This process is autoregressive, generating one token at a time.

âœ… Summary
Component	Description
Autoregressive	Predicts one token at a time, based on past tokens
Decoder-only Model	No encoder, only decoder blocks
Masked Self-Attention	Prevents attention to future tokens
Multi-Head Attention	Multiple attention heads, merged and projected
Feed-Forward Network	Applies deep transformation per token
Embedding + Tokenization	Converts input text to numerical vectors
