# ðŸ“˜ GPT-2 Overview: Autoregressive Model with Masked Self-Attention

## ðŸ§  What Is an Autoregressive Model?

An **autoregressive (AR) model** predicts each token based on all the previous ones. In GPT-2:

P(xâ‚, xâ‚‚, ..., xâ‚™) = P(xâ‚) Â· P(xâ‚‚ | xâ‚) Â· P(xâ‚ƒ | xâ‚, xâ‚‚) Â·Â·Â· P(xâ‚™ | xâ‚, ..., xâ‚™â‚‹â‚)

yaml
å¤åˆ¶
ç¼–è¾‘

This means GPT-2 generates text **one token at a time**, from left to right, and **never looks ahead**.

---

## ðŸ¤– GPT-2 Architecture

- GPT-2 is a **generative model based on the Transformer architecture**
- It **only uses the Decoder blocks** (no Encoder)
- The Decoder is repeated **N times** (e.g., 12 blocks for GPT-2 small)
- GPT-2 is fully **autoregressive**, optimized for **text generation**

---

## ðŸ”„ GPT-2 Tokenization and Embedding Pipeline

### 1. Tokenization

Input text is broken into smaller chunks called **tokens** using Byte Pair Encoding (BPE). Each token maps to a unique **Token ID** from the vocabulary `V`.

Example:  
`"Hello world!" â†’ ["Hello", " world", "!"] â†’ [15496, 995, 0]`

### 2. Embedding Layer

Each Token ID is converted into a **768-dimensional vector** using a learned embedding matrix `E âˆˆ â„^{|V| Ã— 768}`.

Embedding(Token ID) = E[Token ID]

yaml
å¤åˆ¶
ç¼–è¾‘

This gives us:
- A continuous representation of discrete tokens
- Input shape: `seq_len Ã— d_model`

---

## ðŸ§  Self-Attention in GPT-2

Self-Attention helps the model determine **which tokens should pay attention to which others**. It computes:

Attention(Q, K, V) = Softmax(QKáµ€ / âˆšdâ‚–) Â· V

markdown
å¤åˆ¶
ç¼–è¾‘

Each input token is projected into:
- `Q`: Query
- `K`: Key
- `V`: Value

These are obtained via linear transformations:

Q = X Â· W_Q
K = X Â· W_K
V = X Â· W_V

yaml
å¤åˆ¶
ç¼–è¾‘

For **single-head attention**:  
`W_Q, W_K, W_V âˆˆ â„^{768 Ã— 64}`

For **multi-head attention (12 heads)**:  
`W_Q, W_K, W_V âˆˆ â„^{768 Ã— 768}`

---

## ðŸ§© Multi-Head Attention

In **Multi-Head Attention**, multiple attention "heads" operate in parallel:
- Each head focuses on different types of relationships (e.g., syntactic, semantic)
- The outputs of all heads are concatenated:

Concat(headâ‚, ..., headâ‚â‚‚) âˆˆ â„^{seq_len Ã— 768}

vbnet
å¤åˆ¶
ç¼–è¾‘

Then projected back to original space using:

Final Output = Multi-Head Output Â· W_O
W_O âˆˆ â„^{768 Ã— 768}

yaml
å¤åˆ¶
ç¼–è¾‘

> Note: `W_O` is a trainable parameter learned during model training.

---

## ðŸ”’ Masked Multi-Head Self-Attention

GPT-2 uses **Masked Self-Attention** to ensure **causality**:
- Each token **can only attend to previous tokens**
- Future tokens are **masked out** using a triangular mask:

M_{i,j} =
0 if j â‰¤ i
-âˆž if j > i

yaml
å¤åˆ¶
ç¼–è¾‘

Applied before Softmax to prevent "seeing the future".

**Why?**  
To ensure GPT-2 predicts token `x_t` using only `xâ‚, ..., xâ‚œâ‚‹â‚`.

---

## âš™ï¸ Feed-Forward Network (FFN)

After attention, GPT-2 uses a two-layer **position-wise feed-forward network**:

FFN(x) = GELU(x Â· Wâ‚ + bâ‚) Â· Wâ‚‚ + bâ‚‚

yaml
å¤åˆ¶
ç¼–è¾‘

- Applies to each token independently  
- Allows complex feature transformation  
- Hidden layer size is often 4Ã— the model dimension (e.g., 3072 if `d_model = 768`)

---

## ðŸ” GPT-2 Text Generation Loop

To generate a sentence:
1. Input: `"The weather"`
2. Predict: `"is"`
3. New input: `"The weather is"`
4. Predict: `"nice"`
5. Repeat until stop condition

Each step:
- Embeds current input
- Computes Masked Self-Attention
- Uses output to predict next token

> This process is **autoregressive**, generating one token at a time.

---

## âœ… Summary

| Component                   | Description                                              |
|-----------------------------|----------------------------------------------------------|
| **Autoregressive**          | Predicts one token at a time, based on past tokens       |
| **Decoder-only Model**      | No encoder, only decoder blocks                          |
| **Masked Self-Attention**   | Prevents attention to future tokens                      |
| **Multi-Head Attention**    | Multiple attention heads, merged and projected           |
| **Feed-Forward Network**    | Applies deep transformation per token                    |
| **Embedding + Tokenization**| Converts input text to numerical vectors                 |

---
