{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f75e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "from transformers import LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33befb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME = /home/remote/u1138167/JoeyData/hf_home\n",
      "HF_DATASETS_CACHE = None\n",
      "TRANSFORMERS_CACHE = None\n",
      "HF_HUB_CACHE = None\n",
      "HF_DATASETS_HOME = None\n"
     ]
    }
   ],
   "source": [
    "for var in [\n",
    "    \"HF_HOME\",\n",
    "    \"HF_DATASETS_CACHE\",\n",
    "    \"TRANSFORMERS_CACHE\",\n",
    "    \"HF_HUB_CACHE\",\n",
    "    \"HF_DATASETS_HOME\",  # deprecated\n",
    "]:\n",
    "    print(f\"{var} =\", os.getenv(var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c921b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Config\n",
    "CHUNK_SIZE = 512\n",
    "BUFFER_TEXT_SIZE = 1000  # Number of samples to buffer before tokenizing (tune this)\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cc9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔠 Load tokenizer\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73433430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌊 Load streaming dataset\n",
    "hf_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    data_dir=\"sample/10BT\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db400f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14868862\n"
     ]
    }
   ],
   "source": [
    "# print(len(hf_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b34e9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferedStreamTokenChunkDataset(IterableDataset):\n",
    "    def __init__(self, hf_streaming_dataset, tokenizer, chunk_size, buffer_text_size=10000):\n",
    "        self.dataset = hf_streaming_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "        self.buffer_text_size = buffer_text_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        token_buffer = []\n",
    "\n",
    "        for example in self.dataset:\n",
    "            buffer.append(example[\"text\"])\n",
    "            if len(buffer) >= self.buffer_text_size:\n",
    "                tokenized = self.tokenizer.encode(\n",
    "                    \" \".join(buffer),\n",
    "                )\n",
    "                token_buffer.extend(tokenized)\n",
    "                buffer = []\n",
    "\n",
    "                while len(token_buffer) >= self.chunk_size + 1:\n",
    "                    input_ids = token_buffer[:self.chunk_size]\n",
    "                    target_ids = token_buffer[1:self.chunk_size + 1]\n",
    "\n",
    "                    yield {\n",
    "                        \"inputs\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
    "                    }\n",
    "\n",
    "                    token_buffer = token_buffer[self.chunk_size:]\n",
    "\n",
    "        # Final flush\n",
    "        if buffer:\n",
    "            tokenized = self.tokenizer.encode(\n",
    "                \" \".join(buffer),\n",
    "                return_attention_mask=False,\n",
    "                return_token_type_ids=False,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            token_buffer.extend(tokenized)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        while len(token_buffer) >= self.chunk_size + 1:\n",
    "            input_ids = token_buffer[:self.chunk_size]\n",
    "            target_ids = token_buffer[1:self.chunk_size + 1]\n",
    "\n",
    "            yield {\n",
    "                \"inputs\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
    "            }   \n",
    "\n",
    "            token_buffer = token_buffer[self.chunk_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1dc0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BufferedStreamTokenChunkDataset(\n",
    "    hf_streaming_dataset=hf_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    buffer_text_size=BUFFER_TEXT_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5357844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e985c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^Exception ignored in: ^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^\n",
      "\n",
      "^Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    ^^self._shutdown_workers()    ^\n",
      "^self._shutdown_workers()^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    ^if w.is_alive():^    ^\n",
      "if w.is_alive():^ \n",
      "^  ^  ^  ^  ^  ^  ^^ \n",
      "^^AssertionError^^^: ^^can only test a child process^^\n",
      "^^^^^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^^^\n",
      "\n",
      "^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "^      File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "         assert self._parent_pid == os.getpid(), 'can only test a child process'self._shutdown_workers()\n",
      " \n",
      "     File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "              if w.is_alive():Exception ignored in:   \n",
      "  <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>    \n",
      " ^ ^Traceback (most recent call last):\n",
      " ^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^ ^^     ^^ self._shutdown_workers()^^^^\n",
      "^^^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^^^^^    ^^^^^if w.is_alive():^^^^\n",
      "^^^^ ^^^^ ^^^ ^^^ ^^^ ^^^ ^^^ \n",
      "^^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^^    ^^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^^\n",
      "^^ ^^^ ^^ ^^^ ^^^ ^^^ ^ ^^^ ^^ \n",
      "\n",
      " \n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionErrorAssertionError     : : ^assert self._parent_pid == os.getpid(), 'can only test a child process'^can only test a child process\n",
      "can only test a child process^ ^\n",
      " Exception ignored in: \n",
      "^ <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^  \n",
      "^ Exception ignored in: Traceback (most recent call last):\n",
      "^Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^ \n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      " <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>Traceback (most recent call last):\n",
      "^   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "\n",
      "^ Traceback (most recent call last):\n",
      "^    ^      File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^self._shutdown_workers()self._shutdown_workers()^^\n",
      "^    \n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^self._shutdown_workers()^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^    ^\n",
      "^^    if w.is_alive():^if w.is_alive():^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "\n",
      "^^ \n",
      "^^ ^    ^  ^^if w.is_alive(): ^^\n",
      "  ^^  ^ ^ ^ ^^  ^^^ ^^ ^^^  ^^^ ^^^^  ^^^^^^^^^\n",
      "^^^^AssertionError^^^^^^: \n",
      "^^^can only test a child process  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^\n",
      "^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^^^^^ ^^\n",
      "\n",
      " ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError     : ^ assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process ^\n",
      "\n",
      "  ^    \n",
      "  Exception ignored in:   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "  <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>      \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process' ^ \n",
      "Traceback (most recent call last):\n",
      " ^   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^  ^      ^ self._shutdown_workers()^^ \n",
      "^^   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^ ^    ^ if w.is_alive(): ^^\n",
      " ^^   ^^^ ^^ ^^^ ^^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Exception ignored in: ^^^^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^^^^^^\n",
      "^^^^Traceback (most recent call last):\n",
      "^^^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "\n",
      "^^^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^    ^    ^^self._shutdown_workers()^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "^^\n",
      "\n",
      "AssertionError^^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      " : ^^ can only test a child process    ^^ ^\n",
      "if w.is_alive(): ^ \n",
      "^^ ^^  ^^  Exception ignored in: ^ ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20> \n",
      "\n",
      " \n",
      "AssertionError AssertionError Traceback (most recent call last):\n",
      ": :    File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^can only test a child processcan only test a child process \n",
      "\n",
      "^     ^^self._shutdown_workers()^^\n",
      "^^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^^^^    ^^^if w.is_alive():^^^\n",
      "^^ ^^ ^^ ^^ ^\n",
      " ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "     ^ assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^ ^^ ^^ ^^ ^ ^ ^ ^^ ^^ ^ ^^ ^^\n",
      "^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^^ ^^ \n",
      " ^AssertionError ^ : ^can only test a child process \n",
      " ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError^: ^can only test a child process^\n",
      "^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^\n",
      "Traceback (most recent call last):\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^    ^self._shutdown_workers()^^\n",
      "\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "AssertionError:     can only test a child processif w.is_alive():\n",
      "\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749249 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732309 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (770669 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (784781 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749460 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821895 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (788426 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (853391 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5adcbe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(one_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7303b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(one_batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bb2b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "print(one_batch['inputs'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0483156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   91,   860,   287, 11579,  3962,  5659,    25, 57049, 28257,   369,\n",
      "          279, 10563,   315,  7552,   220,   806,   339,  7511,    91,    43,\n",
      "          321,  8651, 41691,   220,    16,   220,   679,    18,    11,   220,\n",
      "         2545,    25,  2970,  6912,  7511,  8161,   956,  2512,   922, 60470,\n",
      "        17146, 12315, 32801,   268, 12278,   268,    13,  4418,   956,  2512,\n",
      "          922,  8388,    72,    11,  2216,    11,   719, 16026,   430,   584,\n",
      "          636,  1063,  1695,   330, 87434,  2891, 17455,   480, 37420,  3001,\n",
      "            1,  2947, 49121, 16290, 40720,  5518,   704,   315,   433,    13,\n",
      "         1628,   330,   943, 15456,     1, 16024,    13,  5321,    13,  1442,\n",
      "         1193,   627,   790, 15229, 55994,  3001, 27597,    37, 55994,    11,\n",
      "        27597,    37, 55994,    11, 27597,    37, 55994, 17523,   551,  1557,\n",
      "          261,   512,    91, 69780, 28257,   369,   279, 10563,   315,  7552,\n",
      "          220,   806,   339,  9787, 83931,    25,  5513,    11, 57049, 28257,\n",
      "          612, 36613,    91,   353,    82,  1108,     9, 92539,   380,  4029,\n",
      "           11,  1095,   757,  1522,   389,  1063,  9650,   311,   499,   358,\n",
      "         9687,   505,   279, 38914,  4633,  4029,   512,  2746,   499,   617,\n",
      "          743,  6261,   389,  4027,    11,   656,   539,  1629,   627, 33413,\n",
      "           30, 36539,  7801,  5321,  5380, 10596,    11,   423,    11,   499,\n",
      "         1047,  1403,  4038,   311,  2019,   311, 25996,   304,   879, 14633,\n",
      "           11,   330,  1453, 14931,    11,   358,  1288,   406,   617,  1027,\n",
      "         1701,   430, 10571,   304,   856,  7318, 38480,    13,   358, 40464,\n",
      "         1005,   433,  1578,    13,   358,  2216,   656,  1093,   364, 32697,\n",
      "         9601,   518,  3582,    11,   323,  1053,  3021,   311,  1005,   433,\n",
      "          304, 24978, 38480,    11,   505,   279, 24432,  3198,   919,  2816,\n",
      "           11,   422,   430,   374, 22281, 10246,    40,  7731,  1618,    11,\n",
      "          369,  1403,  4038,    11,  8748,   369,   430,   311,  3621,    11,\n",
      "         4205,   311,  3621,    11,   323,   433, 48707,    13,  9220,  4038,\n",
      "           11,   389,   701,  1866,  3878,    11,   499,  1436,   617,  5439,\n",
      "          264,  4528,  1772,   311,   379,  5302, 14097,    13,   358,  1053,\n",
      "          617,  2728,   499,   279,  8935,   315,   279, 10712,   313,  7344,\n",
      "          499, 48707,  1440,   279, 20746,  7205,   306,  9621,   311,   279,\n",
      "        10877,    11,   323,   358,  1053,   406,   617,  1071,   264,  3492,\n",
      "         7953,   420,    11,   439,  1202,  5340,    85,  2402,  3575,    11,\n",
      "          539, 10705,    13,  1115,  1053,   617,  1027,   264, 15526, 22380,\n",
      "          311,  1884,   315,   603,  6532,   304, 14892,   449,   499,  1274,\n",
      "           11,   719,   433,  1053,   617,  1027,   264,  8743,  2536, 90465,\n",
      "          369,   499,   627,  4071,  1306,   499,   743,  6261,   389,  4027,\n",
      "           11,   499, 48707,   294,  1559,   433,   704,   449,   264, 15994,\n",
      "          315, 10054,  9439,  8903,   323, 39242,    13,  1472, 10837,    13,\n",
      "         1628, 71175,  2103,  4401,   627, 10445,   539,  1120,  1614,   330,\n",
      "           40, 57445,   709,    13, 33386,  5127,  1210,   323,  3351,   389,\n",
      "           30,  8595,   279, 55586,    30,  8595,   279, 35013,    30,  8595,\n",
      "          279, 28979,  4221,    30,  8595,   279, 40146,  4339,   323, 14774,\n",
      "          292,  6864,  8200,    30,  8595,   279, 14238,   323,   312, 60179,\n",
      "          315,   701,   364,  1911,  4984, 71090,  1226,  1440,   433,   574,\n",
      "          406,  5439,   315,   701,  1866,  4499,   684,    11,   477,   433,\n",
      "         1053,   617,  1027,  2884,   353, 15145,     9, 25996,  1047,   311,\n",
      "         1935,  1957,    13,  1628,    11,   701,  7865,  1603,   420,    11,\n",
      "         9002,   420,  4360,    11,   374,   539, 64005,   315,  4423,   889,\n",
      "         1903,   459, 25226, 16930,    13, 11699, 16682,   627,  4516,  1148,\n",
      "          449,   420, 89706,  4401,    30,  2209,   279, 38550,   311,  2019,\n",
      "          330,    40])\n"
     ]
    }
   ],
   "source": [
    "print(one_batch['inputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "400f25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = one_batch['inputs'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "961e678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd261752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Viewing Single Post From: Spoilers for the Week of February 11th|\n",
      "|Lil||Feb 1 2013, 09:58 AM|\n",
      "Don't care about Chloe/Taniel/Jen-Jen. Don't care about Sami, really, but hoping that we get some good \"SAMANTHA GENE!!\" Marlena Death-Stares out of it. And \"newfound\" feelings. Please. If only.\n",
      "STEFANO!! STEFANO, STEFANO, STEFANO!!!! :cheer:\n",
      "|Spoilers for the Week of February 11th · DAYS: News, Spoilers & Discussion| *sigh* Fundamentalist community, let me pass on some advice to you I learned from the atheistic community:\n",
      "If you have set yourself on fire, do not run.\n",
      "Okay? Okay?? Please?\n",
      "Look, D, you had two months to say to Harvard in private emails, \"Im sorry, I shouldnt have been using that animation in my paid presentations. I wont use it again. I really do like 'Inner Life', though, and would love to use it in classroom presentations, from the BioVisions site, if that is acceptable.\"\n",
      "I sat here, for two months, waiting for that to happen, anything to happen, and it didnt. Two months, on your own terms, you could have written a similar post to yesterdays. I would have given you the benefit of the doubt-- maybe you didnt know the credits werent visible to the audience, and I wouldnt have said a word beyond this, as its Harvards problem, not mine. This would have been a funny joke to those of us involved in dealing with you people, but it would have been a PR non-issue for you.\n",
      "But after you set yourself on fire, you didnt douse it out with a bucket of ice cold reality and accountability. You ran. And youre still running.\n",
      "Why not just state \"I screwed up. Sorry everyone.\" and move on? Why the excuses? Why the denial? Why the passive language? Why the vague words and cryptic capitalizations? Why the writes and rewrites of your 'press release'? We know it wasnt written of your own volition, or it would have been done *before* Harvard had to take action. And, your behavior before this, regarding this issue, is not indicative of someone who made an innocent mistake. Its weird.\n",
      "So what with this frantic running? Is the inability to say \"I\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0ed76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(\"Token count:\", len(enc.encode(decoded_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c36abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6ea28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
