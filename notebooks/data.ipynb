{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f75e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import tiktoken\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33befb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME = /home/remote/u1138167/JoeyData/hf_home\n",
      "HF_DATASETS_CACHE = None\n",
      "TRANSFORMERS_CACHE = None\n",
      "HF_HUB_CACHE = None\n",
      "HF_DATASETS_HOME = None\n"
     ]
    }
   ],
   "source": [
    "for var in [\n",
    "    \"HF_HOME\",\n",
    "    \"HF_DATASETS_CACHE\",\n",
    "    \"TRANSFORMERS_CACHE\",\n",
    "    \"HF_HUB_CACHE\",\n",
    "    \"HF_DATASETS_HOME\",  # deprecated\n",
    "]:\n",
    "    print(f\"{var} =\", os.getenv(var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c921b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Config\n",
    "CHUNK_SIZE = 512\n",
    "BUFFER_TEXT_SIZE = 1000  # Number of samples to buffer before tokenizing (tune this)\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cc9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî† Load tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73433430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de58ef99070e4c5cb5095cbd5204a9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üåä Load streaming dataset\n",
    "hf_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    data_dir=\"sample/10BT\",\n",
    "    split=\"train\",\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db400f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14868862\n"
     ]
    }
   ],
   "source": [
    "# print(len(hf_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34e9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferedStreamTokenChunkDataset(IterableDataset):\n",
    "    def __init__(self, hf_streaming_dataset, tokenizer, chunk_size, buffer_text_size=10000):\n",
    "        self.dataset = hf_streaming_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "        self.buffer_text_size = buffer_text_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        token_buffer = []\n",
    "\n",
    "        for example in self.dataset:\n",
    "            buffer.append(example[\"text\"])\n",
    "            if len(buffer) >= self.buffer_text_size:\n",
    "                tokenized = self.tokenizer.encode(\n",
    "                    \" \".join(buffer),\n",
    "                    allowed_special=self.tokenizer.special_tokens_set\n",
    "                )\n",
    "                token_buffer.extend(tokenized)\n",
    "                buffer = []\n",
    "\n",
    "                while len(token_buffer) >= self.chunk_size + 1:\n",
    "                    input_ids = token_buffer[:self.chunk_size]\n",
    "                    target_ids = token_buffer[1:self.chunk_size + 1]\n",
    "\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
    "                    }\n",
    "\n",
    "                    token_buffer = token_buffer[self.chunk_size:]\n",
    "\n",
    "        # Final flush\n",
    "        if buffer:\n",
    "            tokenized = self.tokenizer.encode(\n",
    "                \" \".join(buffer),\n",
    "                allowed_special=self.tokenizer.special_tokens_set\n",
    "            )\n",
    "            token_buffer.extend(tokenized)\n",
    "\n",
    "        \n",
    "        while len(token_buffer) >= self.chunk_size + 1:\n",
    "            input_ids = token_buffer[:self.chunk_size]\n",
    "            target_ids = token_buffer[1:self.chunk_size + 1]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
    "            }   \n",
    "\n",
    "            token_buffer = token_buffer[self.chunk_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1dc0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BufferedStreamTokenChunkDataset(\n",
    "    hf_streaming_dataset=hf_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    buffer_text_size=BUFFER_TEXT_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5357844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e985c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5adcbe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(one_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b7dd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   91,   860,   287,  ...,  2019,   330,    40],\n",
      "        [  574,  5076,     1,  ...,   339,   969,    13],\n",
      "        [  578, 81960,  1131,  ...,   279, 43732,  4430],\n",
      "        ...,\n",
      "        [  323, 43641,   323,  ..., 89595, 14238, 14134],\n",
      "        [  922, 55182,  7694,  ...,    18,     8,   482],\n",
      "        [64477,  4476, 69131,  ...,   482,  2650,  5195]])\n"
     ]
    }
   ],
   "source": [
    "print(one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400f25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = one_batch[10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "961e678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd261752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " likely to vote tomorrow, Thursday, on the repeal of the FCC‚Äôs Net Neutrality power grab. Using the Congressional Review Act, the repeal of the Net Neutrality order can be accomplished in an expedited way. In particular this means the bill cannot be filibustered in the Senate, so passing it means something. As Seton Motley said: This is our first opportunity | Read More ¬ª Game Index |\n",
      "Deeper into the DarklandsYour Next Campaign picks up the action at Act II, in Beneath a Granite Sky, Part II.\n",
      "[ Read FAQ | Subscribe to RSS | Partner Sites | Contact Us | Advertise with Us ]\n",
      "Copyright ¬© 1996-2009 Skotos Tech, Inc. & individual authors, All Rights Reserved\n",
      "Compilation copyright ¬© 1996-2009 Skotos Tech, Inc.\n",
      "RPGnet¬Æ is a registered trademark of Skotos Tech, Inc., all rights reserved. Great decorating addition\n",
      "I have a grape/Italian theme in my kitchen. I purchased 5 of these. I decided to use them to put around my pull knobs on my overhead cabinets. Now I am ordering more to sprinkle around in other places in the kitchen - even to hang up via suction cups on my white kitchen tile.\n",
      "September 20, 2012 Bible-black with a blinding white logo raging across the chest. It‚Äôs the time honoured Black Band Tee. Every band has one. If you‚Äôre in a band and you ain‚Äôt got a Black Band Tee then you ain‚Äôt even in a band, you‚Äôre in a sham! And if you‚Äôre a fan of a band and you don‚Äôt own the Black Band Tee then what kind of fan are you? Hey?? Sort it out!! Grab yourself a tees worth of black cotton power and put it to the test. Good for you.\n",
      "White as the driven snow, with a filthy black logo centre stage, this tee is cut to make you look like you just got back from the marina where you burned your billion dollar super-yaught down to the waterline just to freak the snot out of all the other billionaires. Good for you.\n",
      "Want to be a Civil Civic model? Send us photos of yourself/friends/animals in a CC Tee and we might just use it here. No matter what you do, it just won‚Äôt stop ‚Äî and you like it.\n",
      "It‚Äôs not your mom‚Äôs relentless text messages (unfailingly signed ‚ÄúLove, Mom‚Äù), the chocolates your boyfriend sends to your cubicle daily (you wish),\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0ed76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(\"Token count:\", len(enc.encode(decoded_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c36abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6ea28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
