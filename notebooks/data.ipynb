{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f75e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "from transformers import LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33befb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME = /home/remote/u1138167/JoeyData/hf_home\n",
      "HF_DATASETS_CACHE = None\n",
      "TRANSFORMERS_CACHE = None\n",
      "HF_HUB_CACHE = None\n",
      "HF_DATASETS_HOME = None\n"
     ]
    }
   ],
   "source": [
    "for var in [\n",
    "    \"HF_HOME\",\n",
    "    \"HF_DATASETS_CACHE\",\n",
    "    \"TRANSFORMERS_CACHE\",\n",
    "    \"HF_HUB_CACHE\",\n",
    "    \"HF_DATASETS_HOME\",  # deprecated\n",
    "]:\n",
    "    print(f\"{var} =\", os.getenv(var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c921b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Config\n",
    "CHUNK_SIZE = 512\n",
    "BUFFER_TEXT_SIZE = 1000  # Number of samples to buffer before tokenizing (tune this)\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cc9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔠 Load tokenizer\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73433430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌊 Load streaming dataset\n",
    "hf_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    data_dir=\"sample/10BT\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db400f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14868862\n"
     ]
    }
   ],
   "source": [
    "# print(len(hf_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b34e9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferedStreamTokenChunkDataset(IterableDataset):\n",
    "    def __init__(self, hf_streaming_dataset, tokenizer, chunk_size, buffer_text_size=10000):\n",
    "        self.dataset = hf_streaming_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "        self.buffer_text_size = buffer_text_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        token_buffer = []\n",
    "\n",
    "        for example in self.dataset:\n",
    "            buffer.append(example[\"text\"])\n",
    "            if len(buffer) >= self.buffer_text_size:\n",
    "                tokenized = self.tokenizer(\n",
    "                    \" \".join(buffer),\n",
    "                    return_attention_mask=False,\n",
    "                    return_token_type_ids=False,\n",
    "                    add_special_tokens=False,\n",
    "                )[\"input_ids\"]\n",
    "                token_buffer.extend(tokenized)\n",
    "                buffer = []\n",
    "\n",
    "                while len(token_buffer) >= self.chunk_size + 1:\n",
    "                    input_ids = token_buffer[:self.chunk_size]\n",
    "                    target_ids = token_buffer[1:self.chunk_size + 1]\n",
    "\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
    "                    }\n",
    "\n",
    "                    token_buffer = token_buffer[self.chunk_size:]\n",
    "\n",
    "        # Final flush\n",
    "        if buffer:\n",
    "            tokenized = self.tokenizer(\n",
    "                \" \".join(buffer),\n",
    "                return_attention_mask=False,\n",
    "                return_token_type_ids=False,\n",
    "                add_special_tokens=False,\n",
    "            )[\"input_ids\"]\n",
    "            token_buffer.extend(tokenized)\n",
    "\n",
    "        while len(token_buffer) >= self.chunk_size + 1:\n",
    "            input_ids = token_buffer[:self.chunk_size]\n",
    "            target_ids = token_buffer[1:self.chunk_size + 1]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "            token_buffer = token_buffer[self.chunk_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1dc0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BufferedStreamTokenChunkDataset(\n",
    "    hf_streaming_dataset=hf_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    buffer_text_size=BUFFER_TEXT_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5357844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e985c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "       self._shutdown_workers() \n",
      "   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "      if w.is_alive(): \n",
      "        ^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^^ ^ ^ ^^ ^ ^\n",
      " AssertionError :  can only test a child process\n",
      "   ^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "^Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^^    self._shutdown_workers()^\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^    ^if w.is_alive():\n",
      "^ ^ ^  ^ ^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError :   can only test a child process \n",
      "     Exception ignored in:   <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^^    ^self._shutdown_workers()^\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^    ^^if w.is_alive():^\n",
      "^ ^ ^ ^ ^^ ^ ^ ^^^^^^^^^^^^^^^^^^^\n",
      "^^AssertionError: ^^can only test a child process\n",
      "\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "      self._shutdown_workers() \n",
      "   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "     if w.is_alive(): \n",
      "           ^ ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "^ ^ ^ ^ ^ ^ ^ ^^  ^ Exception ignored in:  ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>Exception ignored in: ^^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "^Traceback (most recent call last):\n",
      "^^\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^^Traceback (most recent call last):\n",
      "^^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    ^self._shutdown_workers()^\n",
      "^^      File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^^self._shutdown_workers()^    ^\n",
      "^if w.is_alive():\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "\n",
      "^AssertionError      : ^if w.is_alive(): can only test a child process ^\n",
      " \n",
      " ^   ^^Exception ignored in:  ^^ <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^ ^\n",
      "^ Traceback (most recent call last):\n",
      "^^   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^^^^    ^^^self._shutdown_workers()^^^\n",
      "^^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^^^    ^^^if w.is_alive():^^^\n",
      "^\n",
      " ^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^ ^     ^assert self._parent_pid == os.getpid(), 'can only test a child process' ^\n",
      "\n",
      " ^  AssertionError^  ^\n",
      " : ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      " ^can only test a child process    ^ \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'^ \n",
      " ^   ^ ^  ^   ^  ^ ^^ \n",
      " ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      " ^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^^^^ ^^ ^^^ ^^^ ^^^^ ^^^ ^^ ^^^ ^^ ^^^^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "^AssertionError^: AssertionError^can only test a child process: ^can only test a child process\n",
      "^\n",
      "^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^Exception ignored in: \n",
      "^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>Traceback (most recent call last):\n",
      "^\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^^Traceback (most recent call last):\n",
      "^      File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^self._shutdown_workers()^^    \n",
      "^self._shutdown_workers()  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^\n",
      "\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    AssertionError    if w.is_alive():: if w.is_alive():\n",
      "\n",
      "can only test a child process  \n",
      "           ^^ ^^^^^^^^^^^^^^^^^^^^^\n",
      "^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "      File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "AssertionErrorAssertionError: can only test a child process: \n",
      "can only test a child processException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>self._shutdown_workers()\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    if w.is_alive():    \n",
      "self._shutdown_workers() \n",
      "   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "       if w.is_alive(): \n",
      "Exception ignored in:   ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^ \n",
      " ^Traceback (most recent call last):\n",
      " ^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      " ^     ^self._shutdown_workers()\n",
      " ^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^^^    ^^if w.is_alive():^\n",
      "^^ ^^^ \n",
      "^ ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^     ^assert self._parent_pid == os.getpid(), 'can only test a child process' ^^\n",
      " \n",
      "    File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      " ^    ^ assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      " ^  ^^  ^  ^ ^ ^  ^    ^ ^ ^\n",
      " ^ ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^^^    ^^^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^^^\n",
      "^^ ^^^ ^ ^^^ ^^ ^^ ^^ ^^^ ^^^ ^^^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^AssertionError^^: ^\n",
      "can only test a child process^\n",
      "AssertionError^^: ^^can only test a child process^^\n",
      "^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^^^    ^self._shutdown_workers()\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^^    ^if w.is_alive():^^\n",
      "^^ ^^ ^^ ^ ^^ ^^ ^\n",
      "AssertionError : ^can only test a child process\n",
      "^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^    ^self._shutdown_workers()^\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^    if w.is_alive():^\n",
      "\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "      assert self._parent_pid == os.getpid(), 'can only test a child process'  \n",
      "     ^^ ^ ^^ ^ ^ ^ ^ ^^  ^^\n",
      "^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "^ ^ ^^ ^ Exception ignored in:  ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^ \n",
      "^ Traceback (most recent call last):\n",
      "^ ^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^ ^     ^ ^self._shutdown_workers()^^\n",
      "^^^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^^^^    Exception ignored in: ^^^^if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^^^\n",
      "^\n",
      "^^^ Traceback (most recent call last):\n",
      "^^ ^^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      " \n",
      "^AssertionError^     : ^can only test a child processself._shutdown_workers() ^\n",
      "\n",
      " ^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      " ^^^    ^^if w.is_alive():^Exception ignored in: ^\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^^ \n",
      "^ Traceback (most recent call last):\n",
      "^^   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^^ ^    ^ ^self._shutdown_workers()^^ \n",
      "^^^   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^^^    ^^^\n",
      "if w.is_alive():AssertionError^^\n",
      "\n",
      ":  ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      " ^can only test a child process ^     \n",
      "^assert self._parent_pid == os.getpid(), 'can only test a child process' ^ \n",
      "^ ^ ^^^ ^^ \n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^      ^assert self._parent_pid == os.getpid(), 'can only test a child process' ^\n",
      "  ^  ^  ^  ^  ^^ ^ ^^ ^ \n",
      " ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      " ^^    ^^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^^^^ ^^ ^ ^^ ^^ ^^ ^^ ^^ ^^^ ^^ ^^^ ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^AssertionError^: ^^can only test a child process^^\n",
      "^^^^^^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^^\n",
      "\n",
      "^Traceback (most recent call last):\n",
      "AssertionError  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "^: ^can only test a child process    \n",
      "^self._shutdown_workers()^^\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>    ^if w.is_alive():^\n",
      "\n",
      "^Traceback (most recent call last):\n",
      "^ \n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      " AssertionError :      can only test a child process self._shutdown_workers()\n",
      " \n",
      "Exception ignored in:    File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^\n",
      "    ^Traceback (most recent call last):\n",
      "if w.is_alive():^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "\n",
      "^     ^self._shutdown_workers() ^\n",
      "^    File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^  ^    ^ if w.is_alive():^\n",
      "^^^ \n",
      " ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "     ^ assert self._parent_pid == os.getpid(), 'can only test a child process' ^\n",
      "  ^  ^ ^ ^^ ^ ^^ ^ ^^ ^^ \n",
      "^   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^^ ^\n",
      " ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      " ^     ^ assert self._parent_pid == os.getpid(), 'can only test a child process'^ \n",
      "^   ^  ^    ^^ ^ ^^ ^^^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^AssertionError^^: ^^can only test a child process^^\n",
      "^^^Exception ignored in: ^^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>^^\n",
      "^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "AssertionError^^: ^    can only test a child process^self._shutdown_workers()\n",
      "^\n",
      "^  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7d37a77aaa20>    \n",
      "^if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      " AssertionError :      self._shutdown_workers()can only test a child process \n",
      "\n",
      "   File \"/home/remote/u1138167/JoeyLLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1646, in _shutdown_workers\n",
      "      ^if w.is_alive():^\n",
      "^ ^ ^ ^ ^ ^ ^ ^^^^^^\n",
      "^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^ ^ ^\n",
      "   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "      assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "             ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "AssertionErrorAssertionError: : can only test a child processcan only test a child process\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (788425 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732308 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (770668 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (784780 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749459 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749248 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821894 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (853390 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5adcbe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(one_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7303b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(one_batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bb2b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "print(one_batch['input_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0483156",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mone_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mKeyError\u001b[39m: 'inputs'"
     ]
    }
   ],
   "source": [
    "print(one_batch['inputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "400f25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = one_batch['input_ids'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "961e678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd261752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Viewing Single Post From: Spoilers for the Week of February 11th|\n",
      "|Lil||Feb 1 2013, 09:58 AM|\n",
      "Don't care about Chloe/Taniel/Jen-Jen. Don't care about Sami, really, but hoping that we get some good \"SAMANTHA GENE!!\" Marlena Death-Stares out of it. And \"newfound\" feelings. Please. If only.\n",
      "STEFANO!! STEFANO, STEFANO, STEFANO!!!! :cheer:\n",
      "|Spoilers for the Week of February 11th · DAYS: News, Spoilers & Discussion| *sigh* Fundamentalist community, let me pass on some advice to you I learned from the atheistic community:\n",
      "If you have set yourself on fire, do not run.\n",
      "Okay? Okay?? Please?\n",
      "Look, D, you had two months to say to Harvard in private emails, \"Im sorry, I shouldnt have been using that animation in my paid presentations. I wont use it again. I really do like 'Inner Life', though, and would love to use it in classroom presentations, from the BioVisions site, if that is acceptable.\"\n",
      "I sat here, for two months, waiting for that to happen, anything to happen, and it didnt. Two months, on your own terms, you could have written a similar post to yesterdays. I would have given you the benefit of the doubt-- maybe you didnt know the credits werent visible to the audience, and I wouldnt have said a word beyond this, as its Harvards problem, not mine. This would have been a funny joke to those of us involved in dealing with you people, but it would have been a PR non-issue for you.\n",
      "But after you set yourself on fire, you didnt douse it out with a bucket of ice cold reality and accountability. You ran. And youre still running.\n",
      "Why not just state \"I screwed up. Sorry everyone.\" and move on? Why the excuses? Why the denial? Why the passive language? Why the vague words and cryptic capitalizations? Why the writes and rewrites of your 'press release'? We know it wasnt written\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0ed76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")  # or your custom tokenizer\n",
    "print(\"Token count:\", len(tokenizer(decoded_text, add_special_tokens=False)[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c36abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6ea28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
